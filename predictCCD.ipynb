{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"predictCCD","provenance":[],"collapsed_sections":[],"machine_shape":"hm","mount_file_id":"1UKtZLMPSe7BhqiJWnuWgB5gSIo0XLcJX","authorship_tag":"ABX9TyNvETT4Fvu4MsqrNidwEBal"},"kernelspec":{"display_name":"Python 3","name":"python3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"b630d793f4714e7eb83c6fd68d8be1a6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_9422244533bf4b449105d0ecd50379e0","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_9819ce7e0e2949bfa6500b16f62892e3","IPY_MODEL_c5d14206f94445e79db8fad163660cfa"]}},"9422244533bf4b449105d0ecd50379e0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9819ce7e0e2949bfa6500b16f62892e3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_506683fd87ca43c39a18c661c7bb0a8f","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":400000,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":400000,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6a400de95802488ebf0d0ef69d0587e0"}},"c5d14206f94445e79db8fad163660cfa":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_cf3de341583e4b4e93a20446781d16d9","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"â€‹","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 400000/400000 [00:22&lt;00:00, 17744.68it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_609fb9c592684f16a2396b80e33bca8d"}},"506683fd87ca43c39a18c661c7bb0a8f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"6a400de95802488ebf0d0ef69d0587e0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"cf3de341583e4b4e93a20446781d16d9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"609fb9c592684f16a2396b80e33bca8d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"id":"yPk54N648OuP","executionInfo":{"status":"ok","timestamp":1605979107086,"user_tz":300,"elapsed":327,"user":{"displayName":"zixuan zhao","photoUrl":"","userId":"16714798905292107392"}}},"source":["import tensorflow as tf\n","import string,os,glob\n","import tensorflow.keras.callbacks\n","from tensorflow.keras.applications import MobileNet\n","\n","from tensorflow.keras.applications.inception_v3 import InceptionV3\n","from tensorflow.keras.applications.inception_v3 import preprocess_input\n","from tensorflow.keras.preprocessing.image import img_to_array\n","from tqdm import tqdm_notebook\n","from tensorflow.keras.preprocessing.image import load_img\n","import pickle\n","from time import time\n","import numpy as np\n","from PIL import Image\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM, Bidirectional,Concatenate,Embedding, TimeDistributed, Dense, RepeatVector,Activation, Flatten, Reshape, concatenate, Dropout\n","from tensorflow.keras.optimizers import Adam, RMSprop\n","from tensorflow.keras import Input\n","from tensorflow.keras import optimizers\n","from tensorflow.keras.models import Model\n","from tensorflow.keras import layers\n","\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.utils import to_categorical\n","import matplotlib.pyplot as plt\n","import random,copy\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","from tensorflow.keras.models import load_model"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"id":"JAg62p1s9s60","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605979107267,"user_tz":300,"elapsed":501,"user":{"displayName":"zixuan zhao","photoUrl":"","userId":"16714798905292107392"}},"outputId":"7aac0f5e-1232-419b-bdd6-732d6bad850c"},"source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"execution_count":22,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"syWN9bwl-gHu","executionInfo":{"status":"ok","timestamp":1605979107267,"user_tz":300,"elapsed":496,"user":{"displayName":"zixuan zhao","photoUrl":"","userId":"16714798905292107392"}}},"source":["base_folder = '/content/drive/My Drive/20FallMLfinal/Project'\n","data_folder = '/content/drive/My Drive/20FallMLfinal/Project/Data'"],"execution_count":23,"outputs":[]},{"cell_type":"code","metadata":{"id":"Nxijnv6Kvi2n","executionInfo":{"status":"ok","timestamp":1605979107268,"user_tz":300,"elapsed":493,"user":{"displayName":"zixuan zhao","photoUrl":"","userId":"16714798905292107392"}}},"source":["#!unzip \"/content/drive/My Drive/20FallMLfinal/Project/Data/GCCdata_Dataset/GCCdata_Dataset.zip\" -d \"/content/drive/My Drive/20FallMLfinal/Project/Data/GCCdata_Dataset/\""],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"id":"UleM7YcFKjgW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605979107468,"user_tz":300,"elapsed":688,"user":{"displayName":"zixuan zhao","photoUrl":"","userId":"16714798905292107392"}},"outputId":"b0d8c3c9-8a8b-45ec-c4d1-df339b08c1af"},"source":["!ls ../../Data/GCCmodels-dataset/\n"],"execution_count":25,"outputs":[{"output_type":"stream","text":["full-30k-dataset.pkl\t  sizedown-20k-dataset.pkl\n","sizedown-15k-dataset.pkl  sizedown-25k-dataset.pkl\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jAprg3F9BQls","executionInfo":{"status":"ok","timestamp":1605979107468,"user_tz":300,"elapsed":682,"user":{"displayName":"zixuan zhao","photoUrl":"","userId":"16714798905292107392"}}},"source":["import pickle\n","def loadmodel(mname):\n","    f=open(mname,\"rb\")\n","    lookup=pickle.load(f)\n","    maxlength=0\n","    for v in lookup.values():\n","        for i in v:\n","            maxlength=max(maxlength,len(i.split(' ')))\n","    \n","    # create the unique word set\n","    lex = set()\n","    for k in lookup.keys():\n","        [lex.update(d.split()) for d in lookup[k]]\n","    return lookup,lex,maxlength"],"execution_count":26,"outputs":[]},{"cell_type":"code","metadata":{"id":"ougMn4CRocdy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605979107469,"user_tz":300,"elapsed":678,"user":{"displayName":"zixuan zhao","photoUrl":"","userId":"16714798905292107392"}},"outputId":"794b6b26-cee4-45c2-a4d3-bc2fbde8f982"},"source":["import psutil\n","\n","def memcheck(tag=\"\"):\n","    mem = psutil.virtual_memory()\n","    print(\"------\"+tag+\" MEM CHECK------\")\n","    print(\"total: \", round(float(mem.total) / 1024 / 1024 / 1024,20),\"GB\")\n","    print(\"used: \", round(float(mem.used) / 1024 / 1024 / 1024,20),\"GB\")\n","    print(\"free: \", round(float(mem.free) / 1024 / 1024 / 1024,20),\"GB\")\n","    print(\"------END MEM CHECK------\")\n","memcheck(\"test\")"],"execution_count":27,"outputs":[{"output_type":"stream","text":["------test MEM CHECK------\n","total:  12.715892791748047 GB\n","used:  2.0622787475585938 GB\n","free:  5.504764556884766 GB\n","------END MEM CHECK------\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"l3swupuNBUfT","executionInfo":{"status":"ok","timestamp":1605979107655,"user_tz":300,"elapsed":859,"user":{"displayName":"zixuan zhao","photoUrl":"","userId":"16714798905292107392"}}},"source":["import tensorflow.keras.backend as K\n","import sys\n","def data_generator(descriptions, photos, wtoi, max_length, num_photos_batch_size):\n","    # training data for images\n","    # caption\n","    # rest of the caption\n","    # global yield_values\n","    x1,x2,y = [],[],[]\n","    print(\"in gener,lendic \",len(descriptions))\n","    memcheck(\"start gener\")\n","    counting=0\n","    for key,desc_list in descriptions.items():\n","        counting+=1\n","\n","        photo = photos[key]\n","        for desc in desc_list:\n","            # seq is sequence id\n","            seq = [wtoi[word] for word in desc.split() if word in wtoi]\n","            # add the start and end indicator\n","            seq.insert(0,wtoi[START])\n","            seq.append(wtoi[END])\n","\n","            for i in range(1,len(seq)):\n","                in_seq, out_seq = seq[:i],seq[i]\n","                in_seq = pad_sequences([in_seq],maxlen=max_length)[0]\n","                # the out_seq is one-hot encoding\n","                out_seq = to_categorical([out_seq],num_classes=vocab_size)[0]\n","                x1.append(photo)\n","                x2.append(in_seq)\n","                y.append(out_seq)\n","\n","    gc.collect()\n","    memcheck(\"after loop\")\n","    print(\"out gener,lenx:\",len(x1), sys.getsizeof(x1),sys.getsizeof(x2),sys.getsizeof(y))\n","    npx1=np.array(x1)\n","    del x1\n","    npx2=np.array(x2)\n","    del x2\n","    npy=np.array(y)\n","    del y\n","    gc.collect()\n","    #memcheck(\"after y\")\n","    return [[npx1,npx2],npy]\n","    #return [[np.array(x1),np.array(x2)], np.array(y)]\n","\n","\n","            # print([[np.array(x1).shape,np.array(x2).shape], np.array(y).shape])\n","            # yield_values = [[np.array(x1),np.array(x2)], np.array(y)]\n","            # yield [[np.array(x1),np.array(x2)], np.array(y)]"],"execution_count":28,"outputs":[]},{"cell_type":"code","metadata":{"id":"Kh7huG2hBUhz","executionInfo":{"status":"ok","timestamp":1605979107656,"user_tz":300,"elapsed":857,"user":{"displayName":"zixuan zhao","photoUrl":"","userId":"16714798905292107392"}}},"source":["def buildmodel(structid):\n","    if structid==1:\n","        WIDTH,HEIGHT,OUTPUT_DIM=299,299,2048\n","        inputs1 = Input(shape=(OUTPUT_DIM,),name=\"image_features\") # image features\n","        fe1 = Dropout(0.5,name=\"img_dropout\")(inputs1)\n","        fe2 = Dense(512,activation='relu',name=\"img_dense\")(fe1)\n","        inputs2 = Input(shape=(max_length,),name=\"captions\") # gradually accending caption\n","        se1 = Embedding(vocab_size,embedding_dim,mask_zero=True,name=\"glove_embedding\")(inputs2)\n","        se2 = Dropout(0.5,name=\"text_dropout\")(se1)\n","        se3 = Bidirectional(LSTM(256,name=\"text_LSTM\"))(se2)\n","        decorder1 = layers.add([fe2,se3],name=\"merge\") # 256\n","        decorder2 = Dense(256,activation='relu',name=\"merge_dense\")(decorder1)\n","        outputs = Dense(vocab_size,activation='softmax',name=\"prob_distribution\")(decorder2)\n","        caption_model = Model(inputs=[inputs1,inputs2], outputs=outputs)\n","    elif structid==2:\n","        STRUCT=2\n","        WIDTH,HEIGHT,OUTPUT_DIM=299,299,2048\n","        inputs1 = Input(shape=(OUTPUT_DIM,),name=\"image_features\") # image features\n","        fe1 = Dropout(0.5,name=\"img_dropout\")(inputs1)\n","        fe2 = Dense(128,activation='relu',name=\"img_dense\")(fe1)\n","        inputs2 = Input(shape=(max_length,),name=\"captions\") # gradually accending caption\n","        se1 = Embedding(vocab_size,embedding_dim,mask_zero=True,name=\"glove_embedding\")(inputs2)\n","        se2 = Dropout(0.5,name=\"text_dropout\")(se1)\n","        se3 = LSTM(128,name=\"text_LSTM\")(se2)\n","        decorder1 = Concatenate()([fe2,se3])\n","        decorder2 = Dense(256,activation='relu',name=\"merge_dense\")(decorder1)\n","        outputs = Dense(vocab_size,activation='softmax',name=\"prob_distribution\")(decorder2)\n","        caption_model = Model(inputs=[inputs1,inputs2], outputs=outputs)\n","    elif structid==3:\n","        STRUCT=3\n","        WIDTH,HEIGHT,OUTPUT_DIM=299,299,2048\n","        inputs1 = Input(shape=(OUTPUT_DIM,),name=\"image_features\") # image features\n","        fe1 = Dropout(0.5,name=\"img_dropout\")(inputs1)\n","        fe2 = Dense(256,activation='relu',name=\"img_dense\")(fe1)\n","        inputs2 = Input(shape=(max_length,),name=\"captions\") # gradually accending caption\n","        se1 = Embedding(vocab_size,embedding_dim,mask_zero=True,name=\"glove_embedding\")(inputs2)\n","        se2 = Dropout(0.5,name=\"text_dropout\")(se1)\n","        se3 = Bidirectional(LSTM(128,name=\"text_LSTM\"))(se2)\n","        decorder1 = Concatenate()([fe2,se3])\n","        decorder2 = Dense(256,activation='relu',name=\"merge_dense\")(decorder1)\n","        outputs = Dense(vocab_size,activation='softmax',name=\"prob_distribution\")(decorder2)\n","        caption_model = Model(inputs=[inputs1,inputs2], outputs=outputs)\n","    else:#4\n","        WIDTH,HEIGHT,OUTPUT_DIM=299,299,2048\n","        inputs1 = Input(shape=(OUTPUT_DIM,),name=\"image_features\") # image features\n","        fe1 = Dropout(0.5,name=\"img_dropout\")(inputs1)\n","        fe2 = Dense(128,activation='relu',name=\"img_dense\")(fe1)\n","        inputs2 = Input(shape=(max_length,),name=\"captions\") # gradually accending caption\n","        se1 = Embedding(vocab_size,embedding_dim,mask_zero=True,name=\"glove_embedding\")(inputs2)\n","        se2 = Dropout(0.5,name=\"text_dropout\")(se1)\n","        se3 = LSTM(128,name=\"text_LSTM\",return_sequences=True)(se2)\n","        se4 = LSTM(128,name=\"text_LSTM_2\")(se3)\n","        decorder1 = Concatenate()([fe2,se4])\n","        decorder2 = Dense(256,activation='relu',name=\"merge_dense\")(decorder1)\n","        outputs = Dense(vocab_size,activation='softmax',name=\"prob_distribution\")(decorder2)\n","        caption_model = Model(inputs=[inputs1,inputs2], outputs=outputs)\n","    return caption_model"],"execution_count":29,"outputs":[]},{"cell_type":"code","metadata":{"id":"kWye1cZaBUkr","executionInfo":{"status":"ok","timestamp":1605979107656,"user_tz":300,"elapsed":854,"user":{"displayName":"zixuan zhao","photoUrl":"","userId":"16714798905292107392"}}},"source":["import gc,pickle\n","from tensorflow import keras\n","class LossHistory(keras.callbacks.Callback):\n","    def on_train_begin(self, logs={}):\n","        self.losses = []\n"," \n","    def on_batch_end(self, batch, logs={}):\n","        self.losses.append(logs.get('loss'))\n","\n","def train_model(caption_model,batchsize,model_path,modelid,mname,EPOCHS = 10,):\n","    history = LossHistory()\n","    checkpointpath=working_base+\"/zzx/zzx-model/GCC/checkpoints/model_{epoch:02d}-{val_loss:.2f}.h5\"\n","    checkpoint = ModelCheckpoint(checkpointpath, \n","                                 monitor='val_loss', \n","                                 verbose=0, \n","                                 save_best_only=False, \n","                                 save_weights_only=False,\n","                                 period=10)\n","    callbacks_list = [checkpoint]\n","\n","    values = data_generator(train_id_to_captions,encoding_dict,word_to_id,max_length,num_pics_per_batch)\n","    gc.collect()\n","    # caption_model.fit_generator(generator,epochs=1,steps_per_epoch=steps)\n","    print(\"fitting\")\n","    print(\"sizecheck: \",len(values[0][0]),len(values[0][1]),len(values[1]))\n","    caption_model.fit(values[0],values[1],batch_size=batchsize,epochs=EPOCHS,validation_split=0.1,verbose=1,callbacks=[history,checkpoint])#,callbacks=callbacks_list)\n","    gc.collect()\n","\n","    caption_model.save(model_path)\n","    with open(working_base+\"/zzx/zzx-loss/GCC/\"+str(MODELID)+\"-\"+mname[:5]+\"-bat\"+str(num_pics_per_batch)+\"-ep\"+str(EPOCHS)+\".pkl\",'wb') as f:\n","        pickle.dump(history.losses,f)\n","        \n","    print(\"saved\")\n","    "],"execution_count":30,"outputs":[]},{"cell_type":"code","metadata":{"id":"tOuqJITaBUnr","executionInfo":{"status":"ok","timestamp":1605979107656,"user_tz":300,"elapsed":850,"user":{"displayName":"zixuan zhao","photoUrl":"","userId":"16714798905292107392"}}},"source":["def generateCaption(image):\n","    in_text = START\n","    for i in range(max_length):\n","        id_seq = [word_to_id[w] for w in in_text.split() if w in word_to_id.keys()]\n","        id_seq = pad_sequences([id_seq],maxlen=max_length)\n","        yhat = caption_model.predict([image,id_seq],verbose=0)\n","        yhat = np.argmax(yhat)\n","        word = id_to_word[yhat]\n","        in_text += ' ' + word\n","        if word == END:\n","            break\n","    output = in_text.split()\n","    output = output[1:-1]\n","    output = ' '.join(output)\n","    return output"],"execution_count":31,"outputs":[]},{"cell_type":"code","metadata":{"id":"H5-K1a5HnXIi","executionInfo":{"status":"ok","timestamp":1605979111046,"user_tz":300,"elapsed":4235,"user":{"displayName":"zixuan zhao","photoUrl":"","userId":"16714798905292107392"}}},"source":["inceptionV3 = InceptionV3(weights='imagenet')\n","encoder = Model(inceptionV3.input,inceptionV3.layers[-2].output)\n","WIDTH,HEIGHT,OUTPUT_DIM=299,299,2048\n","def encodeImage(img):\n","    from PIL import Image\n","    img = img.resize((HEIGHT,WIDTH),Image.ANTIALIAS)\n","    x = img_to_array(img)\n","    x = np.expand_dims(x,axis=0)\n","    x = preprocess_input(x)\n","    x = encoder.predict(x)\n","    x = np.reshape(x,OUTPUT_DIM)\n","    return x"],"execution_count":32,"outputs":[]},{"cell_type":"code","metadata":{"id":"iCmcXoKgULw4","executionInfo":{"status":"ok","timestamp":1605979111046,"user_tz":300,"elapsed":4230,"user":{"displayName":"zixuan zhao","photoUrl":"","userId":"16714798905292107392"}}},"source":["os.chdir('/content/drive/My Drive/20FallMLfinal/Project/workspace/zzx')"],"execution_count":33,"outputs":[]},{"cell_type":"code","metadata":{"id":"vbDvPCss9uT7","executionInfo":{"status":"ok","timestamp":1605979111047,"user_tz":300,"elapsed":4228,"user":{"displayName":"zixuan zhao","photoUrl":"","userId":"16714798905292107392"}}},"source":["# import pickle\n","# pfffff=open(\"/content/drive/My Drive/20FallMLfinal/Project/Data/smallmodels-dataset/boy-1837-dataset.pkl\",\"rb\")\n","# checkkkk=pickle.load(pfffff)\n","# print(checkkkk)"],"execution_count":34,"outputs":[]},{"cell_type":"code","metadata":{"id":"BhuYNLQsBUqT","colab":{"base_uri":"https://localhost:8080/","height":134,"referenced_widgets":["b630d793f4714e7eb83c6fd68d8be1a6","9422244533bf4b449105d0ecd50379e0","9819ce7e0e2949bfa6500b16f62892e3","c5d14206f94445e79db8fad163660cfa","506683fd87ca43c39a18c661c7bb0a8f","6a400de95802488ebf0d0ef69d0587e0","cf3de341583e4b4e93a20446781d16d9","609fb9c592684f16a2396b80e33bca8d"]},"executionInfo":{"status":"ok","timestamp":1605979136493,"user_tz":300,"elapsed":29669,"user":{"displayName":"zixuan zhao","photoUrl":"","userId":"16714798905292107392"}},"outputId":"ddd89d04-de4c-43c8-fc9e-a44f1905eba3"},"source":["import gc\n","import os\n","\n","#datasets=[\"womanwomen-1787-dataset.pkl\",\"manmen-3042-dataset.pkl\",\"girl-1736-dataset.pkl\",\"dog-2300-dataset.pkl\",\"boy-1837-dataset.pkl\"]\n","\n","embedding_index = {}\n","\n","with open(os.path.join(f'{data_folder}/glove/glove.6B.200d.txt'),encoding='utf-8') as f:\n","    for line in tqdm_notebook(f.readlines()):\n","        vals = line.split()\n","        word = vals[0]\n","        coefs = np.array(vals[1:],dtype='float32')\n","        embedding_index[word] = coefs\n","\n","print(f\"{len(embedding_index)} word vectors\")\n"],"execution_count":35,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n","Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n","  if __name__ == '__main__':\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b630d793f4714e7eb83c6fd68d8be1a6","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=400000.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","400000 word vectors\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1fDFt90YUoDH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605980435383,"user_tz":300,"elapsed":1328552,"user":{"displayName":"zixuan zhao","photoUrl":"","userId":"16714798905292107392"}},"outputId":"775d06ed-a8c5-4fa4-8521-80189759fbb4"},"source":["epoches=100\n","datasets=[\"sizedown-20k-dataset.pkl\"]\n","for ds in datasets:\n","    lookup,lex,max_length=loadmodel(\"/content/drive/My Drive/20FallMLfinal/Project/Data/GCCmodels-dataset/\"+ds)\n","    \n","    for MODELID in range(1,5): \n","        print(\"---------------------------------------------train model \",MODELID,\" on \",ds)\n","        #dont read\n","        img_path_list = glob.glob(f\"{data_folder}/GCCdata_Dataset/GCCdata_Dataset/*.jpg\")\n","        train_img_names = set(open(f'{data_folder}/GCCdata_text/GCC_8k.trainImages.txt').read().strip().split('\\n'))\n","        test_img_names = set(open(f'{data_folder}/GCCdata_text/GCC_8k.testImages.txt').read().strip().split('\\n'))\n","\n","        #for windows\n","        # train_img_names = [img_path for img_path in img_path_list if img_path.split('\\\\')[-1] in train_img_names]\n","        # test_img_names = [img_path for img_path in img_path_list if img_path.split('\\\\')[-1] in test_img_names]\n","        \n","        #for Colab(Linux)\n","        train_img_names = [img_path for img_path in img_path_list if img_path.split('/')[-1] in train_img_names]\n","        test_img_names = [img_path for img_path in img_path_list if img_path.split('/')[-1] in test_img_names]\n","\n","        #external_base_path = \"./content/drive/Shared drives/AIGroup/BigDataProject\"\n","        #gdrive_path = \"./content/drive/My Drive/Homework_And_Projects/BigData\"\n","\n","        #working_base = external_base_path\n","        working_base= \"/content/drive/My Drive/20FallMLfinal/Project/workspace\"\n","\n","        if not os.path.exists(\"{}/GCC_img_encoded.pkl\".format(working_base)):\n","            encoding_dict = {}\n","            for img_path in tqdm_notebook(train_img_names):\n","                img = load_img(img_path,target_size=(HEIGHT,WIDTH))\n","                id = os.path.basename(img_path).split('.')[0]\n","                encoding_dict[id] = encodeImage(img)\n","            with open(\"{}/GCC_img_encoded.pkl\".format(working_base),'wb') as f:\n","                pickle.dump(encoding_dict,f)\n","            print(\"image pickle is saved. Size=\",len(encoding_dict))\n","        else:\n","            with open(\"{}/GCC_img_encoded.pkl\".format(working_base),'rb') as f:\n","                encoding_dict = pickle.load(f)\n","            print(\"image pickle is loaded\")\n","        print(len(encoding_dict))\n","        oldencoding_dict=copy.copy(encoding_dict)\n","        encoding_dict={}\n","\n","        for k,v in oldencoding_dict.items():\n","            if k in lookup.keys():\n","                encoding_dict[k]=v\n","        del oldencoding_dict\n","        print(\"len of encoding dict: \",len(encoding_dict))\n","        \n","\n","        if not os.path.exists(\"{}/GCC_img_encoded_test.pkl\".format(working_base)):\n","            encoding_test_dict = {}\n","            for img_path in tqdm_notebook(test_img_names):\n","                img = load_img(img_path,target_size=(HEIGHT,WIDTH))\n","                id = os.path.basename(img_path).split('.')[0]\n","                encoding_test_dict[id] = encodeImage(img)\n","            with open(\"{}/GCC_img_encoded_test.pkl\".format(working_base),'wb') as f:\n","                pickle.dump(encoding_test_dict,f)\n","            print(\"image pickle (test) is saved.\")\n","        else:\n","            with open(\"{}/GCC_img_encoded_test.pkl\".format(working_base),'rb') as f:\n","                encoding_test_dict = pickle.load(f)\n","            print(\"image pickle (test) is loaded\")\n","        oldencoding_test_dict=copy.copy(encoding_test_dict)\n","        encoding_test_dict={}\n","        for k,v in oldencoding_test_dict.items():\n","            if k in lookup.keys():\n","                encoding_test_dict[k]=v\n","        del oldencoding_test_dict\n","        print(\"len of test dict: \",len(encoding_test_dict))\n","        \n","        training_captions = []\n","        test_captions = []\n","        train_id_to_captions = {}\n","        test_id_to_captions = {}\n","\n","        train_img_ids = [os.path.basename(img).split('.')[0] for img in train_img_names]\n","        test_img_ids = [os.path.basename(img).split('.')[0] for img in test_img_names]\n","\n","        for id, caption_list in lookup.items():\n","            if id in train_img_ids:\n","                training_captions.extend(caption_list)\n","                train_id_to_captions[id] = caption_list\n","            elif id in test_img_ids:\n","                test_captions.extend(caption_list)\n","                test_id_to_captions[id] = caption_list\n","\n","        len(training_captions),len(test_captions)\n","        randomindex=int(random.random()*(len(train_id_to_captions)-6))\n","        list(train_id_to_captions.values())[randomindex:randomindex+5]\n","        count_threshold = 0\n","        START,END = \"startseq\",\"endseq\"\n","        word_counts = {}\n","        for id,cap in enumerate(training_captions): # for each sentence\n","            for w in cap.split(): # for each word\n","                word_counts[w] = word_counts.get(w,0) + 1\n","        freq_words = [w for w in word_counts if word_counts[w] >= count_threshold]\n","        print(\"{} frequent words\".format(len(freq_words)))\n","        # add two indicators\n","        freq_words.extend([START,END])\n","        id_to_word,word_to_id = {},{}\n","\n","        for id,w in enumerate(freq_words):\n","            word_to_id[w] = id+1\n","            id_to_word[id+1] = w\n","\n","        vocab_size = len(word_to_id) + 1 # \n","        vocab_size\n","        max_length += 2 # add \"startseq\",\"endseq\"\n","        max_length\n","#         embedding_index = {}\n","\n","#         with open(os.path.join(f'{base_folder}/glove.6B.200d.txt'),encoding='utf-8') as f:\n","#             for line in tqdm_notebook(f.readlines()):\n","#                 vals = line.split()\n","#                 word = vals[0]\n","#                 coefs = np.array(vals[1:],dtype='float32')\n","#                 embedding_index[word] = coefs\n","\n","#         print(f\"{len(embedding_index)} word vectors\")\n","        embedding_dim = 200 # the vector length of the embedding\n","\n","        embedding_matrix = np.zeros((vocab_size,embedding_dim))\n","        for word, index in word_to_id.items():\n","            embedding_vector = embedding_index.get(word)\n","            if embedding_vector is not None:\n","                embedding_matrix[index] = embedding_vector\n","        embedding_matrix.shape\n","        caption_model=buildmodel(MODELID)\n","        print(\"modelid: \",MODELID)\n","        caption_model.summary()\n","        from tensorflow.keras.utils import plot_model\n","        print(\"modelid: \",MODELID)\n","        plot_model(caption_model,to_file=\"model\"+str(MODELID)+\"_struct.png\",show_shapes=True,show_layer_names=True,dpi=70)\n","        if MODELID==4:\n","            embedding_layer = caption_model.layers[1]\n","        else:\n","            embedding_layer = caption_model.layers[2]\n","        #embedding_layer = caption_model.layers[2]\n","        embedding_layer.set_weights([embedding_matrix]) # should be a list, like the get_weights()\n","        embedding_layer.trainable = False\n","        caption_model.compile(loss='categorical_crossentropy', optimizer='adam')\n","\n","        num_pics_per_batch = 128\n","        steps = len(training_captions) // num_pics_per_batch\n","        # model_path = \"{}/caption_model.h5\".format(working_base)\n","        model_path = working_base+\"/zzx/zzx-model/GCC/\"+str(MODELID)+\"-\"+ds[:5]+\"-bat\"+str(num_pics_per_batch)+\"-ep\"+str(epoches)+\".h5\"\n","        model_path\n","        print(\"steps: \",steps)\n","\n","        gc.collect()\n","        #load checkpoint\n","        # cppath=working_base+\"/zzx/zzx-model/GCC/checkpoints/model_40-8.21.h5\"\n","        # caption_model.load_weights(cppath)\n","        # epoches=60\n","\n","        #train_model(caption_model,num_pics_per_batch,model_path,MODELID,ds,EPOCHS=epoches)\n","        \n","        # #test\n","        # caption_model.load_weights(model_path)\n","        # num=15\n","        # image_names = np.random.choice(list(encoding_test_dict.keys()),num,False)\n","        # for i in range(num):\n","        #     image_name = image_names[i]\n","        #     image_encoding = encoding_test_dict[image_name].reshape((1,-1))\n","        #     full_image_path = f\"{data_folder}/GCCdata_Dataset/GCCdata_Dataset/{image_name}.jpg\"\n","        #     img = plt.imread(full_image_path)\n","        #     plt.imshow(img)\n","        #     plt.show()\n","        #     print(\"Caption=\",generateCaption(image_encoding))\n","        #     print(\"Answer=\",lookup[image_name])\n","        #     print(\"Img name:\", image_name)\n","        #     print(\"_\"*20)\n","        print(model_path)\n","        caption_model.load_weights(model_path)\n","        output=open(working_base+\"/zzx/zzx-model/GCC/\"+str(MODELID)+\"-\"+ds[:5]+\"-bat\"+str(num_pics_per_batch)+\"-ep\"+str(epoches)+\".csv\",\"w\",encoding=\"UTF-8\")\n","        outstr=\"\"\n","        for k,v in encoding_test_dict.items():\n","              image_name=k\n","              image_encoding = encoding_test_dict[image_name].reshape((1,-1))\n","              full_image_path = f\"{data_folder}/GCCdata_Dataset/GCCdata_Dataset/{image_name}.jpg\"\n","              # img = plt.imread(full_image_path)\n","              # plt.imshow(img)\n","              # plt.show()\n","              #print(\"Caption=\",generateCaption(image_encoding))\n","              #print(\"Answer=\",lookup[image_name])\n","              #print(\"Img name:\", image_name)\n","              #print(\"_\"*20)\n","              caption=generateCaption(image_encoding)\n","              imgname=image_name+\".jpg\"\n","              outstr+=imgname+\",\"+caption+\"\\n\"\n","        output.write(outstr)\n","        output.close()"],"execution_count":36,"outputs":[{"output_type":"stream","text":["---------------------------------------------train model  1  on  sizedown-20k-dataset.pkl\n","image pickle is loaded\n","31158\n","len of encoding dict:  20589\n","image pickle (test) is loaded\n","len of test dict:  773\n","11006 frequent words\n","modelid:  1\n","Model: \"functional_9\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","captions (InputLayer)           [(None, 45)]         0                                            \n","__________________________________________________________________________________________________\n","image_features (InputLayer)     [(None, 2048)]       0                                            \n","__________________________________________________________________________________________________\n","glove_embedding (Embedding)     (None, 45, 200)      2201800     captions[0][0]                   \n","__________________________________________________________________________________________________\n","img_dropout (Dropout)           (None, 2048)         0           image_features[0][0]             \n","__________________________________________________________________________________________________\n","text_dropout (Dropout)          (None, 45, 200)      0           glove_embedding[0][0]            \n","__________________________________________________________________________________________________\n","img_dense (Dense)               (None, 512)          1049088     img_dropout[0][0]                \n","__________________________________________________________________________________________________\n","bidirectional_2 (Bidirectional) (None, 512)          935936      text_dropout[0][0]               \n","__________________________________________________________________________________________________\n","merge (Add)                     (None, 512)          0           img_dense[0][0]                  \n","                                                                 bidirectional_2[0][0]            \n","__________________________________________________________________________________________________\n","merge_dense (Dense)             (None, 256)          131328      merge[0][0]                      \n","__________________________________________________________________________________________________\n","prob_distribution (Dense)       (None, 11009)        2829313     merge_dense[0][0]                \n","==================================================================================================\n","Total params: 7,147,465\n","Trainable params: 7,147,465\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","modelid:  1\n","steps:  160\n","/content/drive/My Drive/20FallMLfinal/Project/workspace/zzx/zzx-model/GCC/1-sized-bat128-ep100.h5\n","---------------------------------------------train model  2  on  sizedown-20k-dataset.pkl\n","image pickle is loaded\n","31158\n","len of encoding dict:  20589\n","image pickle (test) is loaded\n","len of test dict:  773\n","11006 frequent words\n","modelid:  2\n","Model: \"functional_11\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","captions (InputLayer)           [(None, 47)]         0                                            \n","__________________________________________________________________________________________________\n","image_features (InputLayer)     [(None, 2048)]       0                                            \n","__________________________________________________________________________________________________\n","glove_embedding (Embedding)     (None, 47, 200)      2201800     captions[0][0]                   \n","__________________________________________________________________________________________________\n","img_dropout (Dropout)           (None, 2048)         0           image_features[0][0]             \n","__________________________________________________________________________________________________\n","text_dropout (Dropout)          (None, 47, 200)      0           glove_embedding[0][0]            \n","__________________________________________________________________________________________________\n","img_dense (Dense)               (None, 128)          262272      img_dropout[0][0]                \n","__________________________________________________________________________________________________\n","text_LSTM (LSTM)                (None, 128)          168448      text_dropout[0][0]               \n","__________________________________________________________________________________________________\n","concatenate_4 (Concatenate)     (None, 256)          0           img_dense[0][0]                  \n","                                                                 text_LSTM[0][0]                  \n","__________________________________________________________________________________________________\n","merge_dense (Dense)             (None, 256)          65792       concatenate_4[0][0]              \n","__________________________________________________________________________________________________\n","prob_distribution (Dense)       (None, 11009)        2829313     merge_dense[0][0]                \n","==================================================================================================\n","Total params: 5,527,625\n","Trainable params: 5,527,625\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","modelid:  2\n","steps:  160\n","/content/drive/My Drive/20FallMLfinal/Project/workspace/zzx/zzx-model/GCC/2-sized-bat128-ep100.h5\n","---------------------------------------------train model  3  on  sizedown-20k-dataset.pkl\n","image pickle is loaded\n","31158\n","len of encoding dict:  20589\n","image pickle (test) is loaded\n","len of test dict:  773\n","11006 frequent words\n","modelid:  3\n","Model: \"functional_13\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","captions (InputLayer)           [(None, 49)]         0                                            \n","__________________________________________________________________________________________________\n","image_features (InputLayer)     [(None, 2048)]       0                                            \n","__________________________________________________________________________________________________\n","glove_embedding (Embedding)     (None, 49, 200)      2201800     captions[0][0]                   \n","__________________________________________________________________________________________________\n","img_dropout (Dropout)           (None, 2048)         0           image_features[0][0]             \n","__________________________________________________________________________________________________\n","text_dropout (Dropout)          (None, 49, 200)      0           glove_embedding[0][0]            \n","__________________________________________________________________________________________________\n","img_dense (Dense)               (None, 256)          524544      img_dropout[0][0]                \n","__________________________________________________________________________________________________\n","bidirectional_3 (Bidirectional) (None, 256)          336896      text_dropout[0][0]               \n","__________________________________________________________________________________________________\n","concatenate_5 (Concatenate)     (None, 512)          0           img_dense[0][0]                  \n","                                                                 bidirectional_3[0][0]            \n","__________________________________________________________________________________________________\n","merge_dense (Dense)             (None, 256)          131328      concatenate_5[0][0]              \n","__________________________________________________________________________________________________\n","prob_distribution (Dense)       (None, 11009)        2829313     merge_dense[0][0]                \n","==================================================================================================\n","Total params: 6,023,881\n","Trainable params: 6,023,881\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","modelid:  3\n","steps:  160\n","/content/drive/My Drive/20FallMLfinal/Project/workspace/zzx/zzx-model/GCC/3-sized-bat128-ep100.h5\n","---------------------------------------------train model  4  on  sizedown-20k-dataset.pkl\n","image pickle is loaded\n","31158\n","len of encoding dict:  20589\n","image pickle (test) is loaded\n","len of test dict:  773\n","11006 frequent words\n","modelid:  4\n","Model: \"functional_15\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","captions (InputLayer)           [(None, 51)]         0                                            \n","__________________________________________________________________________________________________\n","glove_embedding (Embedding)     (None, 51, 200)      2201800     captions[0][0]                   \n","__________________________________________________________________________________________________\n","image_features (InputLayer)     [(None, 2048)]       0                                            \n","__________________________________________________________________________________________________\n","text_dropout (Dropout)          (None, 51, 200)      0           glove_embedding[0][0]            \n","__________________________________________________________________________________________________\n","img_dropout (Dropout)           (None, 2048)         0           image_features[0][0]             \n","__________________________________________________________________________________________________\n","text_LSTM (LSTM)                (None, 51, 128)      168448      text_dropout[0][0]               \n","__________________________________________________________________________________________________\n","img_dense (Dense)               (None, 128)          262272      img_dropout[0][0]                \n","__________________________________________________________________________________________________\n","text_LSTM_2 (LSTM)              (None, 128)          131584      text_LSTM[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate_6 (Concatenate)     (None, 256)          0           img_dense[0][0]                  \n","                                                                 text_LSTM_2[0][0]                \n","__________________________________________________________________________________________________\n","merge_dense (Dense)             (None, 256)          65792       concatenate_6[0][0]              \n","__________________________________________________________________________________________________\n","prob_distribution (Dense)       (None, 11009)        2829313     merge_dense[0][0]                \n","==================================================================================================\n","Total params: 5,659,209\n","Trainable params: 5,659,209\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","modelid:  4\n","steps:  160\n","/content/drive/My Drive/20FallMLfinal/Project/workspace/zzx/zzx-model/GCC/4-sized-bat128-ep100.h5\n"],"name":"stdout"}]}]}